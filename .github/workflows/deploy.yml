name: Deploy to Cloud Run

on:
  push:
    branches:
      - main
  workflow_dispatch:  # Allow manual triggers

env:
  PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
  SERVICE_NAME: psx-research-agent
  REGION: asia-southeast1
  IMAGE_NAME: asia-southeast1-docker.pkg.dev/${{ secrets.GCP_PROJECT_ID }}/psx-research-agent/psx-research-agent

jobs:
  deploy:
    name: Build and Deploy
    runs-on: ubuntu-latest
    permissions:
      contents: read
      id-token: write  # Required for Workload Identity Federation

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          workload_identity_provider: ${{ secrets.WIF_PROVIDER }}
          service_account: github-actions-deployer@${{ secrets.GCP_PROJECT_ID }}.iam.gserviceaccount.com

      - name: Set up Cloud SDK
        uses: google-github-actions/setup-gcloud@v2

      - name: Configure Docker for Artifact Registry
        run: |
          gcloud auth configure-docker ${{ env.REGION }}-docker.pkg.dev --quiet

      - name: Build Docker image
        run: |
          docker build -t $IMAGE_NAME:$GITHUB_SHA -t $IMAGE_NAME:latest .

      - name: Push Docker image
        run: |
          docker push $IMAGE_NAME:$GITHUB_SHA
          docker push $IMAGE_NAME:latest

      - name: Deploy to Cloud Run
        run: |
          gcloud run deploy $SERVICE_NAME \
            --image $IMAGE_NAME:$GITHUB_SHA \
            --platform managed \
            --region $REGION \
            --allow-unauthenticated \
            --set-env-vars OPENROUTER_API_KEY=${{ secrets.OPENROUTER_API_KEY }} \
            --memory 2Gi \
            --cpu 2 \
            --timeout 3600 \
            --max-instances 1 \
            --min-instances 0 \
            --port 8080 \
            --cpu-boost \
            --startup-probe="httpGet.path=/health,httpGet.port=8080,initialDelaySeconds=10,timeoutSeconds=10,periodSeconds=10,failureThreshold=30"

      - name: Clean up old images and orphaned layers
        if: success()
        run: |
          # Keep only the latest 1 image and clean up orphaned layers
          # This ensures we stay within the 500MB free tier limit
          echo "ðŸ§¹ Starting cleanup..."
          
          REPO_PATH="${{ env.REGION }}-docker.pkg.dev/${{ secrets.GCP_PROJECT_ID }}/psx-research-agent"
          
          # Step 1: Delete old images (keep newest 1)
          echo ""
          echo "ðŸ“¦ Step 1: Checking for old images..."
          
          gcloud artifacts docker images list \
            $IMAGE_NAME \
            --format="csv[no-heading](package,version)" \
            --sort-by=~create_time > /tmp/all_images.txt 2>/dev/null || touch /tmp/all_images.txt
          
          sed 's/,/@/' /tmp/all_images.txt > /tmp/images_formatted.txt 2>/dev/null || touch /tmp/images_formatted.txt
          
          IMAGE_COUNT=$(wc -l < /tmp/images_formatted.txt | tr -d ' ')
          
          if [ "$IMAGE_COUNT" -gt 1 ]; then
            KEEP_IMAGE=$(head -1 /tmp/images_formatted.txt)
            echo "Keeping newest image: $KEEP_IMAGE"
            echo "Deleting $((IMAGE_COUNT - 1)) old image(s)..."
            
            while IFS= read -r image_ref; do
              if [ -n "$image_ref" ]; then
                if gcloud artifacts docker images delete "$image_ref" --delete-tags --quiet 2>/dev/null; then
                  echo "  âœ… Deleted: $image_ref"
                fi
              fi
            done < <(tail -n +2 /tmp/images_formatted.txt)
          else
            echo "âœ… Only 1 image exists, no old images to delete."
          fi
          
          rm -f /tmp/all_images.txt /tmp/images_formatted.txt
          
          # Step 2: Clean up orphaned layers
          echo ""
          echo "ðŸ” Step 2: Finding orphaned layers..."
          
          # Get layers used by current image
          docker manifest inspect $IMAGE_NAME:latest 2>/dev/null | python3 -c "
          import json, sys
          data = json.load(sys.stdin)
          if 'config' in data:
              print(data['config']['digest'])
          for layer in data.get('layers', []):
              print(layer['digest'])
          " > /tmp/used_layers.txt 2>/dev/null || touch /tmp/used_layers.txt
          
          # Get all layers in repository
          gcloud artifacts files list \
            --project=${{ secrets.GCP_PROJECT_ID }} \
            --location=${{ env.REGION }} \
            --repository=psx-research-agent \
            --format="value(name)" 2>/dev/null | grep -v "manifests" | sed 's/.*\(sha256:[a-f0-9]*\).*/\1/' > /tmp/all_layers.txt || touch /tmp/all_layers.txt
          
          # Find orphaned layers
          comm -23 <(sort /tmp/all_layers.txt) <(sort /tmp/used_layers.txt) > /tmp/orphaned_layers.txt
          
          ORPHAN_COUNT=$(wc -l < /tmp/orphaned_layers.txt | tr -d ' ')
          
          if [ "$ORPHAN_COUNT" -gt 0 ]; then
            echo "Found $ORPHAN_COUNT orphaned layers. Deleting..."
            
            while IFS= read -r digest; do
              if [ -n "$digest" ]; then
                if gcloud artifacts files delete "$digest" \
                  --project=${{ secrets.GCP_PROJECT_ID }} \
                  --location=${{ env.REGION }} \
                  --repository=psx-research-agent \
                  --quiet 2>/dev/null; then
                  echo "  âœ… Deleted: ${digest:0:30}..."
                fi
              fi
            done < /tmp/orphaned_layers.txt
            
            echo "âœ… Deleted $ORPHAN_COUNT orphaned layers."
          else
            echo "âœ… No orphaned layers found."
          fi
          
          rm -f /tmp/used_layers.txt /tmp/all_layers.txt /tmp/orphaned_layers.txt
          
          echo ""
          echo "âœ… Cleanup complete!"

